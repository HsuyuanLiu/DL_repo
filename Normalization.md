# Normalization

Normalization方法，实际上是一种某种对运行中的层的结果的一种优化方式，在整个训练过程中，较高的层的训练往往会依赖于较前层的训练结果，因此在这一过程中，每一层的训练结果的数据分布特征都会影响后续的训练；因此可以通过对每一层的输出做一次归一化操作，使得某一层的输出变得更为“平滑”，从而完成了对损失函数的平坦性的优化过程。

在这一操作过程中，其所有的优化目标即为：
$$
\overline x_i^r= \frac{x_i^r-m_i}{\sigma_i}
$$
即，其每一层网络的输出应该为对其中某一批次输出的归一化结果（当然，这只是Batch Normalization的做法）。

### Batch Normalization

在批处理的过程中，其对所有输入样本的某一通道的进行计算后进行归一化操作，其意义在于：使得同一批次中的不同样本中的某一通道的输出满足某一高斯分布，这也会导致对其中某一输入的改变进而影响整个批次的平均值与方差的值进而对其它输入造成影响。

### Layer Normalization

在此方法的使用情况下，其主要做法在于对某单个输入数据的各个通道进行归一化操作；即：使得某一输入的不同通道之间的数据更加符合高斯分布而非整个数据批次符合，基于此特点，其主要在批次较小的情况下被使用。

### Instance Normalization

实例归一化，即按照单个样本的单个通道进行计算，计算对象为单层特征图的$N×W$个值。



其基本计算方式如下：



<img src="https://i.imgur.com/ZOyAFf3.png" alt="Imgur" style="zoom:67%;" />



**无论针对哪一种情况，其核心要义在于对于某一层输入的归一化，其区别在于：如何计算平均值乃至方差并作出无差估计：**

- Batch：通过同一批中此通道的输出值进行计算
- Layer：通过该输出的不同通道进行计算
- Instance：只通过该样本的某一通道进行计算，譬如图像的某一通道



对于测试集，其Batch Normalization需要特殊的方法进行，即使用之前训练的加权平均值与方差作为测试时归一化所使用的的平均值以及方差。