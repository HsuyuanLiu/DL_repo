# TF-IDF以及基本机器学习任务

### TF-IDF

可以认为，如果一个词很重要，它应该在该篇文章中多次出现；在这一情况下，可以使用词频`Term Frequency`进行统计操作，即$TF=某一词在一篇文章中出现的次数$。在这一情况下，也可以认识到：一类最常用的词会在几乎每篇文章中大量出现而并不具备统计学意义，**因此可以被过滤掉，这类词又被称为停用词。**

而统计某些出现在特定文本中的一些罕见词就非常有意义，即逆文档频率`Inverse Document Frequency`，其计算方法为：
$$
IDF=lg(\frac{总文档数}{包含该词的文档数+1})
$$


而`TF-IDF`即为$TF-IDF=TF \cdot IDF$

这s方法主要用在机器学习文本识别的过程中而在深度学习任务中，较多使用词嵌入方法。



### 传统方法

所有的`NLP`方法，无论是经典的还是现代的，都以文本数据集开始，也称为语料库(corpora)。语料库中含有与文本相关的原始数据，原始文本是字符(字节)序列，但是大多数时候将字符分组成连续的称为令牌(Tokens)的连续单元是有用的。**在英语中，令牌(Tokens)对应由空格字符或标点分隔的单词和数字序列。**

> 可以将语料库理解为数据集

将文本分解为令牌(Tokens)的过程称为令牌化`tokenization`。在这一初始化过程中，可以按照需要，选取固定长度的连续序列进行初始化令牌的操作：即为`n-gram`方法，



Lemmas是单词的词根形式。考虑动词fly。它可以被屈折成许多不同的单词——flow、fly、flies、flying、flow等等——而fly是所有这些看似不同的单词的Lemmas。为了保持向量表示的维数较低，将令牌减少到它们的Lemmas可能是有用的。这种简化称为`lemmatization`，

在以上处理的基础上，可以进一步的完成对单个单词的标记任务上，即`POS Tagging`操作，

除了对单个词汇的标记，还可以进一步地进行对连续的多令牌边界进行标记乃至对其中的各个部分进行关系的识别与浅解析。

在这一基础上，即可以生成整个句子的解析树。解析树(Parse tree)表示句子中不同的语法单元在层次上是如何相关的。图2-6中的解析树显示了所谓的成分解析。另一种可能更有用的显示关系的方法是使用依赖项解析(dependency parsing)，即直接进行对动宾等结构的标注。

