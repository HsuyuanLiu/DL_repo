# Word2Vec

### 统计语言模型

统计语言模型是用来计算一个句子的概率的概率模型，它通常基于一个语料库来构建。不妨假设$W=(w_1,w_2...w_T)$为由T个词$w_1,w_2...w_T$按顺序构成的一个句子，则其联合分布概率为：
$$
p(W)=p(w_1,w_2...w_T)
$$
 `p(W)`被称为语言模型，即用来计算这个句子概率的模型。利用Bayes公式，上式可以被链式地分解为：
$$
p(W)=p(w_1)p(w_2|w_1)p(w_3|w1,w2)...p(w_T|w_1,w_2...w_{T-1})
$$
其中的条件概率$p(w_1),p(w_2|w_1),p(w_3|w1,w2)$​​等即为语言模型的参数，若这些参数已经全部算得，那么给定一个句子W，就可以很快地计算出相应地概率p(W)了。

在具体的参数计算中，使用：
$$
p(w_T|w_1,w_2...w_{T-1})=\frac{p(w_1,...w_k)}{p(w_1,...w_{k-1})}
$$


#### N-gram

但是，在这一情况下，所需要学习的参数过多，因此需要引入N-gram方法进行计算。假定一个词出现的概率只与它前面固定数目的词相关

即可以认为：
$$
p(w_T|w_1,w_2...w_{T-1}) \approx p(w_T|w_{T-n+1},w_{T-n+2}...w_{T-1})
$$
在这一情况下，实际应用中最多是采用n=3的三元模型。

n-gram模型主要工作是在语料中统计各种词串出现的次数以及平滑化处理。概率值计算好之后就存储起来，下次需要计算一个句子的概率时，只需找到相关的概率参数，将它们连乘起来就好了。

而统计语言模型的优化目标，即是将：
$$
\prod_{w \in C} p(w|Context(w))
$$
进行最大化计算。

### 神经网络模型

与统计模型不同，神经网络模型采取了另外一种方式进行预测，即通过使用词向量的方式进行计算，其输入为k元文法$w_{1:k} $​，输出是下一个词的概率分布，k个词的上下文被当作一个单词窗口，每个词w和词嵌入$v(w) \in R^{d_w}$相对应，输入向量是这k个词的拼接$x=[v(w_1);...v(w_k)]$。

输入的x被传给一个多层感知器进行后续的概率计算。

### 词向量计算

一种最简单的词向量方式是`One-Hot`编码 ，就是用一个很长的向量来表示一个词，向量的长度为词典的大小，向量中只有一个 1 ， 其他全为 0 ，1 的位置对应该词在词典中的位置。

但是这一方法维度太高且无法表示单词的具体意义，因此需要引入Distributed Representation，其基本想法是：通过训练将某种语言中的每一个词 映射成一个固定长度的短向量（当然这里的“短”是相对于One-Hot Representation的“长”而言的），所有这些向量构成一个词向量空间，而每一个向量则可视为 该空间中的一个点，在这个空间上引入“距离”，就可以根据词之间的距离来判断它们之间的语法、语义上的相似性了。

不过，在实际模型中，输入层将可以使用随机编码而非one-hot编码以减小参数数量。

##### 计算

不妨以最简单的模型为例进行计算，即由一个输入的单词对输出的概率进行输出单词的计算。

令词典的大小为$V$，隐藏层的大小为$N$​，且层之间使用全连接层进行连接；

<img src="https://files.mdnice.com/user/18116/4a736a6c-4db4-459b-a78f-63648c2ed44e.PNG" style="zoom:100%;" />

在这里，输入层为one-hot编码，在经过一个$W_{V \times N}$的矩阵后，得到了隐藏层，实际上，在这一步即是完成了对词向量的嵌入，即one-hot矩阵即是完成了对某一单词在$W$矩阵的查找，不妨令$W$矩阵的第$i$行为$v^T_{w_I}$，则可以得到$h=W^TX=v^T_{w_I}$​；

在此基础上，可以从隐藏层进行计算得到输出层中的概率分布：这一步是通过一个不同的权重矩阵$W'$计算得到的
$$
u_j=v^{'T}_{w_j}h
$$
其中$V'_{W_j}$是该剧中$W$的第j列，在此基础上，使用`softmax`函数可以计算出最终的函数分布：
$$
p(w_j|w_I)=y_j=\frac{\exp(u_j)}{\sum_{j'=1}^V\exp(u_{j'})}
$$
第j个词向量的输出概率即为$y_j$。

即可以写为：
$$
p(w_j|w_I)=y_j=\frac{\exp(v^{'T}_{w_j}v_{w_I})}{\sum_{j'=1}^V\exp(v^{'T}_{w_j}v_{w_I})}
$$
其中$v_w$为矩阵$W$的行，$v'_w$为从隐藏层到输出层的矩阵的行。

##### 梯度下降

基于此，即可以完成对该矩阵梯度下降法的优化计算：

可以令优化的目标是求得$p(w_0|w_I)$的最大值，即使得真实输出在给定context下的期望最大：
$$
\max p(w_j|w_I)=\max y_i=\max \log y_{j*}=u_{j*}-\log \sum^V_{j'=1}\exp(u_{j'}):=-E
$$
即此时的损失函数为$E$，而$j*$为实际输出在输出层的索引，因此即可以完成反向计算：
$$
\frac{\partial E}{\partial u_j}=y_j-t_{j}:=e_j
$$
这里，当且仅当$j=j*$时，$t_j=1$，以上公式，可以由$\frac{d}{dx}\log (e^x+b)=\frac{e^x}{e^x+b}$计算得到。

更进一步，可以求出：
$$
\frac{\partial E}{\partial w_{ij}'}=\frac{\partial E}{\partial u_{j}} \cdot \frac{\partial u_j}{\partial w_{ij}'}=e_j \cdot h_i
$$
因此可以完成对$W'$的更新计算：$w^{'(new)}_{ij}=w^{'(old)}_{ij}-\eta \cdot e_j \cdot h_i$，其中$\eta$即为学习率。

同样的，可以使用该方法完成对矩阵$W$的更新。而在这一步对于$W$的优化结果，即可以得到词向量。

#### 多词计算

在此基础上，可以进行多个单词的输入输出计算：

<img src="https://gitee.com/hsuyuanliu/picrepo/raw/master/img/416b7447-fb8b-48a6-a110-04d48fb235c6.PNG" style="zoom:100%;" />

在这一模型中，其基本结构与单一单词输出相同，从输入层到隐藏层的公式改为：
$$
h=\frac{1}{C}W^T(x_1+x_2+....x_C)=\frac{1}{C}(v_{w_1}+...v_{w_{C}})^T
$$
可以发现，所有输入将会共享$W$矩阵，而在这一情况下，可以将损失函数写为：
$$
E=-\log p(w_0|w_{I,1},...w_{I,C})=-{v'}_{w_O}^T \cdot h+ \log \sum_{j'=1}^V\exp({v'}_{w_j}^T \cdot h)
$$
之后即可采取同样的方法进行计算。

这一模型的缺点在于其并没有考虑词序信息，但其在`fasttext`等模型中可以加快计算速度。

##### Skip-gram

`CBOW` 是用上下文预测这个词，而Skip-gram 是预测一个词的上下文。

与之对应，其模型为：

<img src="https://gitee.com/hsuyuanliu/picrepo/raw/master/img/fe01e976-daaf-4d99-8133-9fd10e3b5683.PNG" style="zoom:100%;" />

在这一情况下，$W'$矩阵将会被共享。

### 技巧

`Word2vec` 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练。`Mikolov`主要使用了两种方法：

- `hierarchical softmax`

- - 本质是把 N 分类问题变成 log(N)次二分类

- `negative sampling`

- - 本质是预测总体类别的一个子集

在此方法下，即完成了对计算过程的优化步骤。

